{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab Assignment 2: Evaluate classifiers (10 points)\n",
    " \n",
    "In this assignment you will optimize and compare the perfomance of a parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier on the MNIST dataset.\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline.  \n",
    "\n",
    "### Deadline Friday, November 17, 23:59.\n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "### Criteria used for grading\n",
    "* Explain and analyse all results.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part\n",
    "* Take care that all the figures and tables are well labeled and numbered so that you can easily refer to them.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading points distribution\n",
    "\n",
    "* Implementation 5 points\n",
    "* Results and analysis 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "* Import all necessary modules. Try to use as much of the available functions as possible. \n",
    "* Use the provided train and test set of MNIST dataset.\n",
    "* Pre-process data eg. normalize/standardize, reformat, etc.           \n",
    "  Do whatever you think is necessary and motivate your choices.\n",
    "* (1) Train logistic regression and k-nn using default settings.\n",
    "* Use 10-fold cross validation for each classifier to optimize the performance for one parameter: \n",
    "    * consult the documentation on how cross validation works in sklearn (important functions:             cross_val_score(), GridSearchCV()).\n",
    "    * Optimize k for k-nn,\n",
    "    * for logistic regression focus on the regularization parameter,\n",
    "* (2) Train logistic regression and k-nn using optimized parameters.\n",
    "* Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) and standard deviation,\n",
    "    * plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned. \n",
    "* Compare performance on the test set for two classifiers:\n",
    "    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "    * print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Discuss your results.\n",
    "* BONUS: only continue with this part if you are confident that your implemention is complete \n",
    "    * tune more parameters of logistic regression\n",
    "    * add additional classifiers (NN, Naive Bayes, decision tree), \n",
    "    * analyse additional dataset (ex. Iris dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy\n",
    "import os\n",
    "import struct\n",
    "import math\n",
    "import operator \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse\n",
    "\n",
    "# load MNIST dataset and split in train and test set.\n",
    "# the training set consists of 1500 images of 8x8 pixels, meaning that there are 64 features in total\n",
    "# the test set consists of 297 images\n",
    "# the total amount of images are 1797, 8x8 pixels\n",
    "# input layer is a 1x64 vector, the parameter matrix is 10x64 \n",
    "\n",
    "# The images are stored in byte format, and we will read them into numpy arrays \n",
    "# that we will use to train and test our implementation\n",
    "digits = load_digits()\n",
    "\n",
    "# training and test set images\n",
    "Xtrain = numpy.reshape(digits.images[:1500],(1500,64))\n",
    "Xtest = numpy.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "# training and test set labels; the hand-written numbers range from 0 to 9. Thus there are 10 labels\n",
    "Ytrain = digits.target[:1500]\n",
    "Ytest = digits.target[1500:]\n",
    "\n",
    "# K-NEAREST NEIGHBORS ALGORITHM\n",
    "def distance(instance1, instance2):\n",
    "    distance = 0\n",
    "    # both instances are transformed into an array instead of a matrix \n",
    "    # to make calculation easier\n",
    "    instance1 = numpy.ndarray.flatten(instance1)\n",
    "    instance2 = numpy.ndarray.flatten(instance2)\n",
    "    length = len(instance1) # the length is 64 for all instances\n",
    "    for x in range(length):\n",
    "        distance += pow((instance1[x] - instance2[x]), 2)\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "# The following KNN algorithm can predict the label of an instance. I have \n",
    "# decide that you can choose on which set you want the algorithm to be trained,\n",
    "# which instance you want to predict the label from and also how many neighbors\n",
    "# you want to take into consideration. These are all variables, so feel free\n",
    "# to make this decision yourself. \n",
    "def KNN(training_set, training_target_set, instance_x, target_x, k):\n",
    "    distances = []\n",
    "    targets = []\n",
    "    for i in range(len(training_set)):\n",
    "        dist = distance(instance_x, training_set[i])\n",
    "        # distances is the list with all the distances between instance_x \n",
    "        # and all the instances in the training_set\n",
    "        distances.append(dist)\n",
    "    for j in range(len(training_target_set)):\n",
    "        # enumerates the labels together with the training_set\n",
    "        targets.append(training_target_set[j])\n",
    "    # combined puts the list of distances and targets in one list\n",
    "    combined = numpy.vstack((distances, targets)).T\n",
    "    # sort does a sorting only looking at the distances in combined\n",
    "    # sorting from smallest distances to largest \n",
    "    sort = sorted(combined, key=operator.itemgetter(0))\n",
    "    votes = []\n",
    "    # k decides how many nearest neighbors you want to look at\n",
    "    for i in range(k):\n",
    "        vote = sort[k][1]\n",
    "        votes.append(vote)\n",
    "    counts = numpy.bincount(votes)\n",
    "    prediction = numpy.argmax(counts)\n",
    "    # verifies whether your prediction is true or false\n",
    "    if prediction == target_x:\n",
    "        return True, target_x\n",
    "    if prediction != target_x:\n",
    "        return False\n",
    "\n",
    "# I have decided that the output of this algorithm the label of the instance is\n",
    "# only if the algorithm has made an accurate prediction. If the prediciton is\n",
    "# false, the algorithm will return False. This means that you have to adjust your k\n",
    "\n",
    "# An example of how to run the algorithm is:\n",
    "KNN(Xtrain, Ytrain, digits.images[1532], digits.target[1532], 10)\n",
    "KNN(Xtrain, Ytrain, digits.images[1600], digits.target[1600], 30)\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "mnist = digits\n",
    "batch = Xtrain\n",
    "tb = Xtest\n",
    "\n",
    "\n",
    "def getLoss(w,x,y,lam):\n",
    "    m = x.shape[0] # the number of training examples\n",
    "    y_mat = oneHotIt(y) #Next we convert the integer class coding into a one-hot representation\n",
    "    scores = np.dot(x,w) #computation of the raw class scores given our input and current weights\n",
    "    prob = softmax(scores) #Next we perform a softmax on these scores to get their probabilities\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lam/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lam*w #And compute the gradient for that loss\n",
    "    return loss,grad\n",
    "\n",
    "def oneHotIt(Y):\n",
    "    m = Y.shape[0]\n",
    "    #Y = Y[:,0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1)).T\n",
    "    return sm\n",
    "\n",
    "def getProbsAndPreds(someX):\n",
    "    probs = softmax(np.dot(someX,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "x = Xtrain\n",
    "y = Ytrain\n",
    "w = np.zeros([x.shape[1],len(np.unique(y))])\n",
    "lam = 1\n",
    "iterations = 1000\n",
    "learningRate = 1e-5\n",
    "losses = []\n",
    "for i in range(0,iterations):\n",
    "    loss,grad = getLoss(w,x,y,lam)\n",
    "    losses.append(loss)\n",
    "    w = w - (learningRate * grad)\n",
    "print(loss)\n",
    "\n",
    "plt.plot(losses)\n",
    "\n",
    "def getAccuracy(someX,someY):\n",
    "    prob,prede = getProbsAndPreds(someX)\n",
    "    accuracy = sum(prede == someY)/(float(len(someY)))\n",
    "    return accuracy\n",
    "\n",
    "# here we can print the training accuracy and the test accuracy\n",
    "print('Training Accuracy:', getAccuracy(x,y))\n",
    "print('Test Accuracy:', getAccuracy(Xtest,Ytest))\n",
    "\n",
    "# we can also play around and try to visulize some number such as 3 and 1 \n",
    "classWeightsToVisualize = 3\n",
    "plt.imshow(scipy.reshape(w[:,classWeightsToVisualize],[8,8]))\n",
    "\n",
    "classWeightsToVisualize = 1\n",
    "plt.imshow(scipy.reshape(w[:,classWeightsToVisualize],[8,8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and analysis of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "from sklearn import datasets, svm, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# KNN ALGORITHM OPTIMIZATION OF K AND CROSS VALIDATION\n",
    "mnist = digits\n",
    " \n",
    "# take the MNIST data and construct the training and testing split, using 83.5% of the\n",
    "# data for training and 16.5% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(mnist.data), mnist.target, test_size=0.165, random_state=42)\n",
    " \n",
    "# to use a 10-fold cross-validation we that 10% from the training set \n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels, test_size=0.1, random_state=84)\n",
    " \n",
    "# here the length of the sets are shown \n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))\n",
    "\n",
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "# I only want to look at a number of k that is odd, because if k is odd, you know\n",
    "# for sure that one label is more frequent than any other label. \n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    " \n",
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in xrange(1, 30, 2):\n",
    "    # train the kNN classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(trainData, trainLabels)\n",
    " \n",
    "    # evaluate the model and update the accuracies list\n",
    "    score = model.score(valData, valLabels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)\n",
    " \n",
    "    # find the value of k that has the largest accuracy\n",
    "i = np.argmax(accuracies)\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i], accuracies[i] * 100))\n",
    "\n",
    "# re-train our classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(trainData, trainLabels)\n",
    "predictions = model.predict(testData)\n",
    " \n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))\n",
    "\n",
    "for i in np.random.randint(0, high=len(testLabels), size=(5,)):\n",
    "    image = testData[i]\n",
    "    prediction = model.predict(image)[0]\n",
    " \n",
    "    # In order to see the image 'better', I reshape the image\n",
    "    image = image.reshape((8, 8)).astype(\"uint8\")\n",
    "    image = exposure.rescale_intensity(image, out_range=(0, 255))\n",
    "    image = imutils.resize(image, width=32, inter=cv2.INTER_CUBIC)\n",
    " \n",
    "    # here the prediction is showed \n",
    "    print(\"I think that digit is: {}\".format(prediction))\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "#LOGISTIC REGRESSION\n",
    "print(__doc__)\n",
    "\n",
    "# The MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# Once again I reshape de images to make computation easier \n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# I decided to use a support vector classifier to make it easy to print \n",
    "# a confusion matrix as well as precision, recall, f1-score and support table\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
