{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ML LabExercise - Univariate Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to implement univariate linear regression. You will implement a gradient descent procedure to iteratively search for the solution. \n",
    "$$\n",
    "\\newcommand{\\ls}[1]{{}^{(#1)}}\n",
    "\\renewcommand{\\v}[1]{\\boldsymbol{#1}}\n",
    "\\renewcommand{\\T}{{}^T}\n",
    "\\newcommand{\\matvec}[1]{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $(x\\ls 1,y\\ls 1),\\ldots,(x\\ls m, y\\ls m)$ where the $x$ values are the independent variables, these values are error free. The dependent values $y$ do contain errors.\n",
    "\n",
    "Linear regression fits a model function (*hypothesis*) $h_{\\v\\theta}(x)$ such that the sum of squared errors is minimized:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\v\\theta}(x\\ls i) - y\\ls i)^2\n",
    "$$\n",
    "Linear regression is called *linear* regression because we assume the hypothesis function $h_{\\v\\theta}$ is linear in its parameters $\\v\\theta$:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\theta_0 \\phi_0(x) + \\cdots + \\theta_n \\phi_n(x)\n",
    "$$\n",
    "where $\\phi_0,\\ldots,\\phi_n$ are arbitrary functions in $x$. In case we write:\n",
    "$$\n",
    "\\v x = \\matvec{\\phi_0(x)\\\\\\vdots\\\\\\phi_n(x)}\n",
    "$$\n",
    "the hypothesis function becomes:\n",
    "$$\n",
    "h_{\\v\\theta}(x) = \\v\\theta\\T \\v x\n",
    "$$\n",
    "and the cost function is:\n",
    "$$\n",
    "J(\\v\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i)^2\n",
    "$$\n",
    "The gradient is given by:\n",
    "$$\n",
    "\\frac{\\partial J(\\v\\theta)}{\\partial \\v\\theta} =\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (\\v\\theta\\T\\v x\\ls i - y\\ls i) \\v x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression in Practice I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a simple example. We will generate data with:\n",
    "$$\n",
    "   y\\ls i = a x\\ls i + b + R\n",
    "$$\n",
    "where $R$ is a random variable, i.e. its value is not exactly\n",
    "known. We assume here that $R$ is normally distributed with mean zero\n",
    "and standard deviation 0.3.\n",
    "\n",
    "We collect all values $x\\ls i$ for $i=1,\\ldots,m$ in an array of shape ``(m,)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoCRIbgBYGQIbgAYGYIbAEaG4AaAkfk/25bQZVgdApUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f184e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that $x\\ls i$ is stored in ``x[i]`` (and equivalently for $y$\n",
    "and ``y``).\n",
    "\n",
    "In this case we want to fit a model of the form $h_{\\v\\theta}(x)=a x + b$\n",
    "to the data. Note that with \n",
    "$$\n",
    "   \\v x = \\matvec{1\\\\x}\n",
    "$$\n",
    "(i.e. with $\\phi_0(x)=1$ and $\\phi_1(x)=x$) we have:\n",
    "$$\n",
    "   h_{\\v\\theta}(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "where $\\theta_0$ is $a$ and $\\theta_0$ is $b$. A constant function\n",
    "$\\phi_0$ in a linear hypothesis (*linear in its parameters!*) is often\n",
    "called a bias term of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``cost(theta, x, y)`` that calculates the cost. Note that ``x`` is the vector with all $x\\ls i$-values and ``y`` is the vector with all $y\\ls i$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045 0.0462925150044\n"
     ]
    }
   ],
   "source": [
    "def cost(theta0, theta1, x, y): \n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    result = (np.sum(((theta0+(theta1*x))-y)**2))/(2*len(x))\n",
    "    return result \n",
    "\n",
    "print(0.3**2/2, cost(2, 0.5, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function called with ``cost(b,a,x,y)`` (where ``b``, ``a``,\n",
    "``x`` and ``y``) are defined as in the previous code snippet,\n",
    "should a return a value that is close to $0.3^2/2$ (For extra\n",
    "points: can you prove this?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Write a function ``theta0, theta1 = gradDescentStep(theta0, theta1, x, y)`` that does the calculations for one gradient descent step. In this function we use the Python possibility to return a tuple of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradDescentStep(learningrate, theta0, theta1, x, y):\n",
    "    d_theta0 = sum((theta0+theta1*x)-y)/size(x)\n",
    "    d_theta1 = ((theta0+theta1*x)-y).dot(x)/size(x) \n",
    "    theta0 = theta0-learningrate*d_theta0 # update of theta0\n",
    "    theta1 = theta1-learningrate*d_theta1 # update of theta1, both updates have to be done simultaneously\n",
    "    return theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with values ``theta0 = theta1 = 0``. Calculate the costfor these values. After the gradient descent step, using ``learningrate=0.01``, resulting in new theta values again calculate the cost. If all went well the cost should have decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1265447162 >= 5.08658064545\n"
     ]
    }
   ],
   "source": [
    "theta0 = theta1 = 0\n",
    "costbefore = cost(theta0, theta1, x, y)\n",
    "theta0, theta1 = gradDescentStep(0.01, theta0, theta1, x, y)\n",
    "costafter = cost(theta0, theta1, x, y)\n",
    "print(costbefore, '>=', costafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
